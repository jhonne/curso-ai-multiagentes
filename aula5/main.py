#!/usr/bin/env python3
"""
Aula 5: OtimizaÃ§Ã£o e ConfiguraÃ§Ã£o AvanÃ§ada dos Agentes

Este arquivo demonstra os conceitos principais da aula atravÃ©s de exemplos prÃ¡ticos.
"""

import os
from dotenv import load_dotenv
from crewai import Agent, Task, Crew, Process
import time
import json
from datetime import datetime

# Carrega variÃ¡veis de ambiente
load_dotenv()


class AgenteOtimizado:
    """
    Classe base para criar agentes com configuraÃ§Ãµes otimizadas
    """

    def __init__(
        self,
        role,
        goal,
        backstory,
        model="gpt-3.5-turbo",
        temperature=0.7,
        max_tokens=1000,
    ):
        self.role = role
        self.goal = goal
        self.backstory = backstory
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.metrics = {"tokens_used": 0, "execution_time": 0, "success_rate": 0}

    def create_agent(self, tools=None):
        """Cria um agente CrewAI com configuraÃ§Ãµes otimizadas"""
        return Agent(
            role=self.role,
            goal=self.goal,
            backstory=self.backstory,
            tools=tools or [],
            verbose=True,
            allow_delegation=False,
            llm_config={
                "model": self.model,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
            },
        )

    def log_performance(self, start_time, tokens_used):
        """Registra mÃ©tricas de performance"""
        execution_time = time.time() - start_time
        self.metrics["execution_time"] = execution_time
        self.metrics["tokens_used"] += tokens_used

        print(f"\nðŸ“Š MÃ©tricas do Agente {self.role}:")
        print(f"â±ï¸  Tempo de execuÃ§Ã£o: {execution_time:.2f}s")
        print(f"ðŸ”¤ Tokens utilizados: {tokens_used}")
        print(f"ðŸ’° Custo estimado: ${(tokens_used * 0.002):.4f}")


def exemplo_prompt_engineering():
    """
    Demonstra tÃ©cnicas avanÃ§adas de prompt engineering
    """
    print("\nðŸŽ¯ === EXEMPLO: PROMPT ENGINEERING AVANÃ‡ADO ===")

    # Prompt bÃ¡sico vs otimizado
    prompt_basico = "Analise este texto"

    prompt_otimizado = """
    VocÃª Ã© um analista de conteÃºdo especializado em extrair insights acionÃ¡veis.
    
    CONTEXTO: AnÃ¡lise de feedback de clientes
    OBJETIVO: Identificar padrÃµes e sugestÃµes de melhoria
    
    FORMATO DE RESPOSTA:
    1. Resumo executivo (max 50 palavras)
    2. Principais insights (mÃ¡ximo 3 pontos)
    3. AÃ§Ãµes recomendadas (mÃ¡ximo 3 aÃ§Ãµes)
    
    EXEMPLO:
    Texto: "O produto Ã© bom, mas a entrega demorou muito"
    Resposta:
    1. Cliente satisfeito com produto, insatisfeito com logÃ­stica
    2. Insights: Qualidade mantida, logÃ­stica precisa melhorar
    3. AÃ§Ãµes: Revisar processo de entrega, comunicar prazos melhor
    
    Analise o seguinte texto:
    """

    print(f"âŒ Prompt bÃ¡sico: {prompt_basico}")
    print(f"âœ… Prompt otimizado: {prompt_otimizado[:200]}...")
    print("\nðŸ’¡ DiferenÃ§as:")
    print("- Contexto especÃ­fico")
    print("- Formato estruturado de resposta")
    print("- Exemplo prÃ¡tico (few-shot learning)")
    print("- LimitaÃ§Ãµes claras de tamanho")


def exemplo_configuracao_modelos():
    """
    Demonstra configuraÃ§Ã£o de diferentes modelos para diferentes tipos de agentes
    """
    print("\nâš™ï¸ === EXEMPLO: CONFIGURAÃ‡ÃƒO DE MODELOS ===")

    # ConfiguraÃ§Ãµes por tipo de agente
    configs = {
        "agente_criativo": {
            "model": "gpt-4",
            "temperature": 0.9,  # Alta criatividade
            "max_tokens": 2000,
            "top_p": 0.9,
        },
        "agente_analitico": {
            "model": "gpt-3.5-turbo",
            "temperature": 0.1,  # Baixa variabilidade
            "max_tokens": 1000,
            "top_p": 0.1,
        },
        "agente_conversacional": {
            "model": "gpt-3.5-turbo",
            "temperature": 0.7,  # Equilibrado
            "max_tokens": 500,
            "top_p": 0.8,
        },
    }

    for tipo, config in configs.items():
        print(f"\nðŸ¤– {tipo.upper()}:")
        for param, valor in config.items():
            print(f"  {param}: {valor}")

        # ExplicaÃ§Ã£o da configuraÃ§Ã£o
        if "criativo" in tipo:
            print("  ðŸ“ Foco: GeraÃ§Ã£o de conteÃºdo original e criativo")
        elif "analitico" in tipo:
            print("  ðŸ“Š Foco: AnÃ¡lise precisa e consistente")
        elif "conversacional" in tipo:
            print("  ðŸ’¬ Foco: InteraÃ§Ã£o natural e fluida")


def exemplo_sistema_cache():
    """
    Demonstra implementaÃ§Ã£o de cache para otimizar custos
    """
    print("\nðŸ’¾ === EXEMPLO: SISTEMA DE CACHE ===")

    class CacheSimples:
        def __init__(self):
            self.cache = {}
            self.hits = 0
            self.misses = 0

        def get_hash(self, prompt):
            """Gera hash simples do prompt"""
            return hash(prompt.strip().lower())

        def get(self, prompt):
            """Busca resposta no cache"""
            key = self.get_hash(prompt)
            if key in self.cache:
                self.hits += 1
                print(f"âœ… Cache HIT para prompt: {prompt[:50]}...")
                return self.cache[key]
            else:
                self.misses += 1
                print(f"âŒ Cache MISS para prompt: {prompt[:50]}...")
                return None

        def set(self, prompt, response):
            """Armazena resposta no cache"""
            key = self.get_hash(prompt)
            self.cache[key] = response
            print(f"ðŸ’¾ Resposta armazenada no cache")

        def stats(self):
            """EstatÃ­sticas do cache"""
            total = self.hits + self.misses
            hit_rate = (self.hits / total * 100) if total > 0 else 0
            return {
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate": f"{hit_rate:.1f}%",
            }

    # DemonstraÃ§Ã£o
    cache = CacheSimples()

    prompts_teste = [
        "Qual Ã© a capital do Brasil?",
        "Como fazer um bolo de chocolate?",
        "Qual Ã© a capital do Brasil?",  # Repetido - deve usar cache
        "Explique machine learning",
        "Como fazer um bolo de chocolate?",  # Repetido - deve usar cache
    ]

    for prompt in prompts_teste:
        resposta = cache.get(prompt)
        if not resposta:
            # Simula resposta da API
            resposta = f"Resposta simulada para: {prompt}"
            cache.set(prompt, resposta)

    print(f"\nðŸ“ˆ EstatÃ­sticas do Cache: {cache.stats()}")
    print("ðŸ’¡ Taxa de cache de 40% = 40% de economia em tokens!")


def main():
    """
    FunÃ§Ã£o principal que executa todos os exemplos
    """
    print("ðŸš€ AULA 5: OTIMIZAÃ‡ÃƒO E CONFIGURAÃ‡ÃƒO AVANÃ‡ADA DOS AGENTES")
    print("=" * 70)

    # Executa exemplos
    exemplo_prompt_engineering()
    exemplo_configuracao_modelos()
    exemplo_sistema_cache()

    print("\nâœ… Exemplos concluÃ­dos!")
    print("\nðŸ“š PrÃ³ximos passos:")
    print("1. Execute os exemplos na pasta 'exemplos/'")
    print("2. Complete os exercÃ­cios na pasta 'exercicios/'")
    print("3. Use os templates para seus prÃ³prios agentes")
    print("\nðŸ’¡ Dica: Use sempre verbose=True durante desenvolvimento!")


if __name__ == "__main__":
    main()
