#!/usr/bin/env python3
"""
Exemplo 3: Testes Unit√°rios para Agentes CrewAI
Demonstra como criar e executar testes para validar o comportamento dos agentes
"""

import unittest
from unittest.mock import Mock, patch
from dotenv import load_dotenv
from crewai import Agent, Task, Crew
import time
import json

load_dotenv()


class TesteAgente(unittest.TestCase):
    """Classe base para testes de agentes"""

    def setUp(self):
        """Configura√ß√£o inicial para cada teste"""
        self.agente_teste = Agent(
            role="Agente de Teste",
            goal="Executar testes espec√≠ficos",
            backstory="Agente criado especificamente para testes unit√°rios",
            verbose=False,
            llm_config={
                "model": "gpt-3.5-turbo",
                "temperature": 0.1,  # Baixa para consist√™ncia nos testes
                "max_tokens": 200,
            },
        )

    def test_criacao_agente(self):
        """Testa se o agente √© criado corretamente"""
        self.assertIsNotNone(self.agente_teste)
        self.assertEqual(self.agente_teste.role, "Agente de Teste")
        self.assertEqual(self.agente_teste.goal, "Executar testes espec√≠ficos")

    def test_configuracao_llm(self):
        """Testa se as configura√ß√µes do LLM est√£o corretas"""
        config = self.agente_teste.llm_config
        self.assertEqual(config["model"], "gpt-3.5-turbo")
        self.assertEqual(config["temperature"], 0.1)
        self.assertEqual(config["max_tokens"], 200)


class TestePrompts(unittest.TestCase):
    """Testes espec√≠ficos para valida√ß√£o de prompts"""

    def setUp(self):
        """Configura√ß√£o inicial"""
        self.agente_analise = Agent(
            role="Analista de Sentimento",
            goal="Classificar sentimento de textos",
            backstory="""
            Voc√™ analisa textos e classifica o sentimento como:
            POSITIVO, NEGATIVO ou NEUTRO.
            Responda APENAS com uma dessas palavras.
            """,
            verbose=False,
            llm_config={
                "model": "gpt-3.5-turbo",
                "temperature": 0.0,  # Zero para m√°xima consist√™ncia
                "max_tokens": 10,
            },
        )

    def test_sentimento_positivo(self):
        """Testa classifica√ß√£o de sentimento positivo"""
        texto = "Adorei o produto! Superou todas as expectativas!"

        task = Task(
            description=f"Classifique o sentimento: {texto}",
            expected_output="POSITIVO, NEGATIVO ou NEUTRO",
            agent=self.agente_analise,
        )

        crew = Crew(agents=[self.agente_analise], tasks=[task], verbose=False)

        try:
            resultado = crew.kickoff()
            # Verifica se cont√©m palavra-chave positiva
            self.assertIn("POSITIVO", str(resultado).upper())
        except Exception as e:
            self.fail(f"Teste falhou com erro: {e}")

    def test_sentimento_negativo(self):
        """Testa classifica√ß√£o de sentimento negativo"""
        texto = "Produto terr√≠vel! N√£o recomendo para ningu√©m!"

        task = Task(
            description=f"Classifique o sentimento: {texto}",
            expected_output="POSITIVO, NEGATIVO ou NEUTRO",
            agent=self.agente_analise,
        )

        crew = Crew(agents=[self.agente_analise], tasks=[task], verbose=False)

        try:
            resultado = crew.kickoff()
            self.assertIn("NEGATIVO", str(resultado).upper())
        except Exception as e:
            self.fail(f"Teste falhou com erro: {e}")


class TesteSistemaMonitoramento:
    """Sistema para monitorar performance dos agentes"""

    def __init__(self):
        self.metricas = {
            "execucoes_totais": 0,
            "sucessos": 0,
            "falhas": 0,
            "tempo_total": 0,
            "tokens_utilizados": 0,
        }

    def executar_teste_com_monitoramento(self, agente, task_description):
        """Executa teste e coleta m√©tricas"""
        start_time = time.time()
        self.metricas["execucoes_totais"] += 1

        try:
            task = Task(
                description=task_description,
                expected_output="Resposta estruturada",
                agent=agente,
            )

            crew = Crew(agents=[agente], tasks=[task], verbose=False)
            resultado = crew.kickoff()

            # Simula contagem de tokens (estimativa)
            tokens_estimados = len(str(resultado).split()) * 1.3
            self.metricas["tokens_utilizados"] += tokens_estimados

            execution_time = time.time() - start_time
            self.metricas["tempo_total"] += execution_time
            self.metricas["sucessos"] += 1

            return {
                "sucesso": True,
                "resultado": resultado,
                "tempo": execution_time,
                "tokens": tokens_estimados,
            }

        except Exception as e:
            self.metricas["falhas"] += 1
            execution_time = time.time() - start_time
            self.metricas["tempo_total"] += execution_time

            return {
                "sucesso": False,
                "erro": str(e),
                "tempo": execution_time,
                "tokens": 0,
            }

    def gerar_relatorio(self):
        """Gera relat√≥rio de performance"""
        if self.metricas["execucoes_totais"] == 0:
            return "Nenhum teste executado"

        taxa_sucesso = (
            self.metricas["sucessos"] / self.metricas["execucoes_totais"]
        ) * 100
        tempo_medio = self.metricas["tempo_total"] / self.metricas["execucoes_totais"]

        return f"""
        üìä RELAT√ìRIO DE PERFORMANCE
        ===========================
        ‚Ä¢ Execu√ß√µes totais: {self.metricas['execucoes_totais']}
        ‚Ä¢ Sucessos: {self.metricas['sucessos']}
        ‚Ä¢ Falhas: {self.metricas['falhas']}
        ‚Ä¢ Taxa de sucesso: {taxa_sucesso:.1f}%
        ‚Ä¢ Tempo m√©dio: {tempo_medio:.2f}s
        ‚Ä¢ Tokens totais: {self.metricas['tokens_utilizados']:.0f}
        ‚Ä¢ Custo estimado: ${(self.metricas['tokens_utilizados'] * 0.002):.4f}
        """


def criar_suite_testes():
    """Cria uma su√≠te completa de testes para agentes"""

    print("üß™ CRIANDO SU√çTE DE TESTES")
    print("=" * 40)

    # Agente para testes
    agente_teste = Agent(
        role="Assistente de An√°lise",
        goal="Analisar textos e fornecer insights",
        backstory="""
        Voc√™ √© um assistente especializado em an√°lise de textos.
        Forne√ßa sempre respostas concisas e estruturadas.
        """,
        verbose=False,
        llm_config={"model": "gpt-3.5-turbo", "temperature": 0.2, "max_tokens": 150},
    )

    # Sistema de monitoramento
    monitor = TesteSistemaMonitoramento()

    # Casos de teste
    casos_teste = [
        "Analise este feedback: 'Produto bom, entrega r√°pida'",
        "Resuma em uma frase: 'Cliente comprou 3 itens, pagou √† vista'",
        "Classifique a prioridade: 'Sistema fora do ar h√° 2 horas'",
        "Extraia palavras-chave: 'Vendas aumentaram 20% no trimestre'",
        "Sugira uma a√ß√£o: 'Muitas reclama√ß√µes sobre atendimento'",
    ]

    print("üîÑ Executando casos de teste...")

    resultados = []
    for i, caso in enumerate(casos_teste, 1):
        print(f"   Teste {i}/5: ", end="")
        resultado = monitor.executar_teste_com_monitoramento(agente_teste, caso)

        if resultado["sucesso"]:
            print(f"‚úÖ ({resultado['tempo']:.1f}s)")
        else:
            print(f"‚ùå ({resultado['erro'][:30]}...)")

        resultados.append(resultado)

    # Gera relat√≥rio
    print(monitor.gerar_relatorio())

    return resultados


def demonstrar_debugging_verbose():
    """Demonstra uso do modo verbose para debugging"""

    print("\nüîç DEMONSTRA√á√ÉO: DEBUGGING COM VERBOSE")
    print("=" * 50)

    agente_debug = Agent(
        role="Agente de Debug",
        goal="Demonstrar processo de racioc√≠nio",
        backstory="""
        Voc√™ precisa explicar seu processo de racioc√≠nio passo a passo.
        Seja detalhado sobre como chegou √† conclus√£o.
        """,
        verbose=True,  # Ativa modo debug
        llm_config={"model": "gpt-3.5-turbo", "temperature": 0.5, "max_tokens": 300},
    )

    task = Task(
        description="""
        Analise esta situa√ß√£o e explique seu racioc√≠nio:
        
        Uma loja online teve:
        - 1000 visitantes ontem
        - 50 vendas
        - Ticket m√©dio de R$ 120
        
        A meta √© 8% de convers√£o. A loja atingiu a meta?
        """,
        expected_output="An√°lise com c√°lculos e conclus√£o",
        agent=agente_debug,
    )

    crew = Crew(
        agents=[agente_debug], tasks=[task], verbose=True  # Mostra todo o processo
    )

    print("üîÑ Executando com verbose=True...")
    print("(Voc√™ ver√° todo o processo de racioc√≠nio do agente)")
    print("-" * 50)

    try:
        resultado = crew.kickoff()
        print(f"\n‚úÖ Resultado final:\n{resultado}")
    except Exception as e:
        print(f"‚ùå Erro: {e}")


def validar_qualidade_resposta():
    """Valida a qualidade das respostas dos agentes"""

    print("\n‚úÖ VALIDA√á√ÉO DE QUALIDADE")
    print("=" * 35)

    def avaliar_resposta(resposta, criterios):
        """Avalia resposta baseada em crit√©rios"""
        pontuacao = 0
        feedback = []

        for criterio, peso in criterios.items():
            if criterio == "tamanho" and 50 <= len(str(resposta)) <= 300:
                pontuacao += peso
                feedback.append("‚úÖ Tamanho adequado")
            elif criterio == "estrutura" and (
                ":" in str(resposta) or "." in str(resposta)
            ):
                pontuacao += peso
                feedback.append("‚úÖ Bem estruturada")
            elif criterio == "relevancia" and len(str(resposta).split()) >= 10:
                pontuacao += peso
                feedback.append("‚úÖ Conte√∫do relevante")

        return pontuacao, feedback

    # Crit√©rios de qualidade
    criterios = {
        "tamanho": 30,  # 30% da nota
        "estrutura": 40,  # 40% da nota
        "relevancia": 30,  # 30% da nota
    }

    # Resposta de exemplo
    resposta_teste = """
    An√°lise da situa√ß√£o:
    ‚Ä¢ Taxa de convers√£o atual: 5% (50/1000)
    ‚Ä¢ Meta: 8%
    ‚Ä¢ Conclus√£o: Meta n√£o atingida
    ‚Ä¢ Recomenda√ß√£o: Melhorar processo de venda
    """

    pontuacao, feedback = avaliar_resposta(resposta_teste, criterios)

    print(f"üìä Pontua√ß√£o: {pontuacao}/100")
    print("üìã Feedback:")
    for item in feedback:
        print(f"   {item}")

    if pontuacao >= 80:
        print("üéâ Qualidade EXCELENTE!")
    elif pontuacao >= 60:
        print("üëç Qualidade BOA")
    else:
        print("‚ö†Ô∏è Qualidade precisa melhorar")


def main():
    """Fun√ß√£o principal"""
    print("üöÄ EXEMPLO 3: TESTES UNIT√ÅRIOS PARA AGENTES")
    print("=" * 70)

    # Executa testes unit√°rios b√°sicos
    print("1Ô∏è‚É£ Executando testes unit√°rios b√°sicos...")
    suite = unittest.TestLoader().loadTestsFromTestCase(TesteAgente)
    runner = unittest.TextTestRunner(verbosity=0)
    resultado = runner.run(suite)

    if resultado.wasSuccessful():
        print("‚úÖ Todos os testes b√°sicos passaram!")
    else:
        print("‚ùå Alguns testes falharam")

    # Executa su√≠te de testes completa
    print("\n2Ô∏è‚É£ Executando su√≠te de testes completa...")
    criar_suite_testes()

    # Demonstra debugging
    demonstrar_debugging_verbose()

    # Valida qualidade
    validar_qualidade_resposta()

    print("\nüí° DICAS PARA TESTES:")
    print("1. Use temperature=0 para testes que precisam de consist√™ncia")
    print("2. Limite max_tokens para testes mais r√°pidos")
    print("3. Use verbose=True apenas durante desenvolvimento")
    print("4. Monitore sempre tempo de execu√ß√£o e uso de tokens")
    print("5. Crie crit√©rios objetivos para avaliar qualidade")
    print("\nüìö Pr√≥ximo: Execute 04_monitoramento.py")


if __name__ == "__main__":
    main()
