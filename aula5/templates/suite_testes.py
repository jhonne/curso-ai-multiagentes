#!/usr/bin/env python3
"""
Template: Suite de Testes para Agentes CrewAI
Use este template para criar testes abrangentes para seus agentes
"""

import unittest
import time
from datetime import datetime
from dotenv import load_dotenv
from crewai import Agent, Task, Crew

load_dotenv()


class SuiteTestesAgente(unittest.TestCase):
    """
    Suite completa de testes para agentes CrewAI

    Use este template copiando e adaptando para seus agentes espec√≠ficos
    """

    @classmethod
    def setUpClass(cls):
        """Configura√ß√£o inicial para toda a suite de testes"""
        cls.agente_padrao = cls._criar_agente_teste()
        cls.metricas_teste = {
            "total_testes": 0,
            "sucessos": 0,
            "falhas": 0,
            "tempo_total": 0,
        }

    @classmethod
    def _criar_agente_teste(cls):
        """Cria agente padr√£o para testes"""
        return Agent(
            role="Agente de Teste",
            goal="Executar tarefas de teste de forma consistente",
            backstory="""
            Voc√™ √© um agente especializado em fornecer respostas
            consistentes e estruturadas para testes automatizados.
            Seja preciso e siga exatamente as instru√ß√µes.
            """,
            verbose=False,
            llm_config={
                "model": "gpt-3.5-turbo",
                "temperature": 0.1,  # Baixa para consist√™ncia
                "max_tokens": 200,
            },
        )

    def setUp(self):
        """Configura√ß√£o antes de cada teste"""
        self.start_time = time.time()
        self.__class__.metricas_teste["total_testes"] += 1

    def tearDown(self):
        """Limpeza ap√≥s cada teste"""
        execution_time = time.time() - self.start_time
        self.__class__.metricas_teste["tempo_total"] += execution_time

        # Conta sucesso se o teste passou
        if not self._outcome.errors and not self._outcome.failures:
            self.__class__.metricas_teste["sucessos"] += 1
        else:
            self.__class__.metricas_teste["falhas"] += 1

    # ========================================================================
    # TESTES DE CONFIGURA√á√ÉO
    # ========================================================================

    def test_agente_criado_corretamente(self):
        """Testa se o agente foi criado com configura√ß√µes corretas"""
        self.assertIsNotNone(self.agente_padrao)
        self.assertEqual(self.agente_padrao.role, "Agente de Teste")
        self.assertIsNotNone(self.agente_padrao.goal)
        self.assertIsNotNone(self.agente_padrao.backstory)

    def test_configuracao_llm(self):
        """Testa configura√ß√µes do modelo LLM"""
        config = self.agente_padrao.llm_config
        self.assertIn("model", config)
        self.assertIn("temperature", config)
        self.assertIn("max_tokens", config)

        # Testa valores espec√≠ficos
        self.assertEqual(config["temperature"], 0.1)
        self.assertEqual(config["max_tokens"], 200)

    # ========================================================================
    # TESTES DE FUNCIONALIDADE
    # ========================================================================

    def test_execucao_basica(self):
        """Testa execu√ß√£o b√°sica de uma tarefa"""
        task = Task(
            description="Responda apenas: 'Teste executado com sucesso'",
            expected_output="Confirma√ß√£o de teste",
            agent=self.agente_padrao,
        )

        crew = Crew(agents=[self.agente_padrao], tasks=[task], verbose=False)

        resultado = crew.kickoff()
        self.assertIsNotNone(resultado)
        self.assertIn("sucesso", str(resultado).lower())

    def test_resposta_estruturada(self):
        """Testa se o agente consegue gerar respostas estruturadas"""
        task = Task(
            description="""
            Analise este feedback: 'Produto bom, entrega r√°pida'
            Responda no formato:
            Sentimento: [POSITIVO/NEGATIVO/NEUTRO]
            Aspectos: [lista de aspectos mencionados]
            """,
            expected_output="An√°lise estruturada do feedback",
            agent=self.agente_padrao,
        )

        crew = Crew(agents=[self.agente_padrao], tasks=[task], verbose=False)

        resultado = crew.kickoff()
        resultado_str = str(resultado).upper()

        # Verifica estrutura da resposta
        self.assertIn("SENTIMENTO:", resultado_str)
        self.assertIn("ASPECTOS:", resultado_str)
        self.assertTrue(
            any(
                palavra in resultado_str
                for palavra in ["POSITIVO", "NEGATIVO", "NEUTRO"]
            )
        )

    def test_consistencia_respostas(self):
        """Testa consist√™ncia das respostas para o mesmo prompt"""
        prompt = "Classifique como POSITIVO ou NEGATIVO: 'Produto excelente!'"

        resultados = []
        for _ in range(3):  # Executa 3 vezes
            task = Task(
                description=prompt,
                expected_output="POSITIVO ou NEGATIVO",
                agent=self.agente_padrao,
            )

            crew = Crew(agents=[self.agente_padrao], tasks=[task], verbose=False)

            resultado = crew.kickoff()
            resultados.append(str(resultado).upper())

        # Todas as respostas devem conter "POSITIVO"
        for resultado in resultados:
            self.assertIn("POSITIVO", resultado)

    # ========================================================================
    # TESTES DE PERFORMANCE
    # ========================================================================

    def test_tempo_resposta_aceitavel(self):
        """Testa se o tempo de resposta est√° dentro do aceit√°vel"""
        task = Task(
            description="Responda 'OK' apenas",
            expected_output="Confirma√ß√£o simples",
            agent=self.agente_padrao,
        )

        crew = Crew(agents=[self.agente_padrao], tasks=[task], verbose=False)

        start_time = time.time()
        crew.kickoff()
        execution_time = time.time() - start_time

        # Deve responder em menos de 30 segundos
        self.assertLess(execution_time, 30.0, f"Resposta demorou {execution_time:.2f}s")

    def test_limite_tokens_respeitado(self):
        """Testa se o limite de tokens √© respeitado"""
        task = Task(
            description="Escreva um texto longo sobre intelig√™ncia artificial",
            expected_output="Texto sobre IA",
            agent=self.agente_padrao,
        )

        crew = Crew(agents=[self.agente_padrao], tasks=[task], verbose=False)

        resultado = crew.kickoff()

        # Estimativa simples: ~0.75 tokens por palavra
        palavras = len(str(resultado).split())
        tokens_estimados = palavras * 0.75

        # Deve respeitar o limite de 200 tokens (com margem)
        self.assertLess(
            tokens_estimados, 250, f"Resposta muito longa: ~{tokens_estimados} tokens"
        )

    # ========================================================================
    # TESTES DE ROBUSTEZ
    # ========================================================================

    def test_prompt_vazio(self):
        """Testa comportamento com prompt vazio"""
        task = Task(
            description="", expected_output="Resposta padr√£o", agent=self.agente_padrao
        )

        crew = Crew(agents=[self.agente_padrao], tasks=[task], verbose=False)

        # Deve executar sem erro (mesmo que a resposta seja estranha)
        try:
            resultado = crew.kickoff()
            self.assertIsNotNone(resultado)
        except Exception as e:
            self.fail(f"Agente falhou com prompt vazio: {e}")

    def test_prompt_muito_longo(self):
        """Testa comportamento com prompt muito longo"""
        prompt_longo = "Analise este texto: " + "teste " * 1000  # ~1000 palavras

        task = Task(
            description=prompt_longo,
            expected_output="An√°lise do texto",
            agent=self.agente_padrao,
        )

        crew = Crew(agents=[self.agente_padrao], tasks=[task], verbose=False)

        # Deve lidar com prompt longo sem falhar
        try:
            resultado = crew.kickoff()
            self.assertIsNotNone(resultado)
        except Exception as e:
            # Se falhar, deve ser por limite de tokens, n√£o por erro interno
            self.assertIn("token", str(e).lower())

    # ========================================================================
    # TESTES ESPEC√çFICOS POR DOM√çNIO
    # ========================================================================

    def test_analise_sentimento_positivo(self):
        """Testa an√°lise de sentimento positivo"""
        feedbacks_positivos = [
            "Produto excelente! Recomendo muito!",
            "Servi√ßo maravilhoso, equipe muito atenciosa",
            "Superou minhas expectativas em todos os aspectos",
        ]

        for feedback in feedbacks_positivos:
            with self.subTest(feedback=feedback):
                task = Task(
                    description=f"Classifique o sentimento: '{feedback}'. Responda apenas POSITIVO, NEGATIVO ou NEUTRO.",
                    expected_output="Classifica√ß√£o de sentimento",
                    agent=self.agente_padrao,
                )

                crew = Crew(agents=[self.agente_padrao], tasks=[task], verbose=False)

                resultado = crew.kickoff()
                self.assertIn("POSITIVO", str(resultado).upper())

    def test_analise_sentimento_negativo(self):
        """Testa an√°lise de sentimento negativo"""
        feedbacks_negativos = [
            "Produto terr√≠vel! N√£o funcionou nem um dia!",
            "Atendimento p√©ssimo, muito insatisfeito",
            "Pior compra que j√° fiz, n√£o recomendo",
        ]

        for feedback in feedbacks_negativos:
            with self.subTest(feedback=feedback):
                task = Task(
                    description=f"Classifique o sentimento: '{feedback}'. Responda apenas POSITIVO, NEGATIVO ou NEUTRO.",
                    expected_output="Classifica√ß√£o de sentimento",
                    agent=self.agente_padrao,
                )

                crew = Crew(agents=[self.agente_padrao], tasks=[task], verbose=False)

                resultado = crew.kickoff()
                self.assertIn("NEGATIVO", str(resultado).upper())

    @classmethod
    def tearDownClass(cls):
        """Relat√≥rio final ap√≥s todos os testes"""
        print(f"\n{'='*60}")
        print("üìä RELAT√ìRIO FINAL DOS TESTES")
        print(f"{'='*60}")
        print(f"üìà Total de testes: {cls.metricas_teste['total_testes']}")
        print(f"‚úÖ Sucessos: {cls.metricas_teste['sucessos']}")
        print(f"‚ùå Falhas: {cls.metricas_teste['falhas']}")
        print(f"‚è±Ô∏è Tempo total: {cls.metricas_teste['tempo_total']:.2f}s")

        if cls.metricas_teste["total_testes"] > 0:
            taxa_sucesso = (
                cls.metricas_teste["sucessos"] / cls.metricas_teste["total_testes"]
            ) * 100
            tempo_medio = (
                cls.metricas_teste["tempo_total"] / cls.metricas_teste["total_testes"]
            )

            print(f"üéØ Taxa de sucesso: {taxa_sucesso:.1f}%")
            print(f"‚ö° Tempo m√©dio: {tempo_medio:.2f}s")

            if taxa_sucesso >= 90:
                print("üéâ EXCELENTE! Agente muito confi√°vel!")
            elif taxa_sucesso >= 70:
                print("üëç BOM! Agente funcional com pequenos problemas")
            else:
                print("‚ö†Ô∏è ATEN√á√ÉO! Agente precisa de melhorias")


class TestesRapidos(unittest.TestCase):
    """Suite de testes r√°pidos para desenvolvimento"""

    def setUp(self):
        self.agente = Agent(
            role="Teste R√°pido",
            goal="Respostas r√°pidas para testes",
            backstory="Agente otimizado para testes r√°pidos",
            verbose=False,
            llm_config={
                "model": "gpt-3.5-turbo",
                "temperature": 0.0,
                "max_tokens": 50,  # Bem limitado para velocidade
            },
        )

    def test_resposta_simples(self):
        """Teste r√°pido de resposta simples"""
        task = Task(
            description="Responda apenas 'OK'",
            expected_output="Confirma√ß√£o",
            agent=self.agente,
        )

        crew = Crew(agents=[self.agente], tasks=[task], verbose=False)
        resultado = crew.kickoff()

        self.assertIn("OK", str(resultado).upper())


def executar_suite_completa():
    """Executa suite completa de testes"""
    print("üöÄ EXECUTANDO SUITE COMPLETA DE TESTES")
    print("=" * 70)

    # Carrega todas as classes de teste
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()

    # Adiciona testes
    suite.addTests(loader.loadTestsFromTestCase(SuiteTestesAgente))
    suite.addTests(loader.loadTestsFromTestCase(TestesRapidos))

    # Executa com relat√≥rio detalhado
    runner = unittest.TextTestRunner(verbosity=2)
    resultado = runner.run(suite)

    return resultado


def executar_testes_rapidos():
    """Executa apenas testes r√°pidos para desenvolvimento"""
    print("‚ö° EXECUTANDO TESTES R√ÅPIDOS")
    print("=" * 40)

    suite = unittest.TestLoader().loadTestsFromTestCase(TestesRapidos)
    runner = unittest.TextTestRunner(verbosity=2)
    resultado = runner.run(suite)

    return resultado


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "rapido":
        # Executa apenas testes r√°pidos
        executar_testes_rapidos()
    else:
        # Executa suite completa
        executar_suite_completa()

    print("\nüí° DICAS DE USO:")
    print("‚Ä¢ python suite_testes.py         # Suite completa")
    print("‚Ä¢ python suite_testes.py rapido  # Apenas testes r√°pidos")
    print("‚Ä¢ Adapte os testes para seus agentes espec√≠ficos")
    print("‚Ä¢ Use subTest() para testes com m√∫ltiplos casos")
    print("‚Ä¢ Monitore tempo de execu√ß√£o para otimiza√ß√£o")
